#!/usr/bin/env python3
"""Aggregate reconnaissance scan results into a single inventory file.

This utility reads output generated by different network reconnaissance tools
and merges the findings into a consolidated inventory.  The supported inputs
are:

* ``masscan`` JSON results (default path: ``out/masscan.json``)
* ``nmap`` XML output files (default directory: ``out/nmap``)
* ``theHarvester`` text exports (default directory: ``out/harvester``)

Two files are produced: a JSON inventory and a CSV inventory.  The structure of
the JSON file mirrors the columns in the CSV output, allowing the inventory to
be consumed programmatically or inspected in a spreadsheet.

Example
-------
Run the tool with default paths (useful when executing from the project root)::

    python tools/aggregate.py

Or provide explicit locations for the source and output files::

    python tools/aggregate.py \
        --nmap-dir out/custom-nmap \
        --masscan-json out/custom-masscan.json \
        --harv-dir out/custom-harvester \
        --out-json out/combined/inventory.json \
        --out-csv out/combined/inventory.csv

The script prints the paths of the generated files upon completion.
"""

import argparse
import csv
import ipaddress
import json
import os
import re
import xml.etree.ElementTree as ET
from typing import Dict, Iterable, List, NamedTuple, Optional, Sequence


def parse_masscan_json(path: str) -> Dict[str, Dict[str, List[int]]]:
    """Read a Masscan JSON export and return a mapping of IPs to port numbers.

    Parameters
    ----------
    path:
        Location of the JSON file produced by Masscan.  The file is expected to
        contain a list of objects with at least an ``ip`` key and, optionally, a
        ``ports`` collection.

    Returns
    -------
    dict
        Keys are IP address strings.  Values are dictionaries containing a
        ``masscan_ports`` list with the discovered port numbers in ascending
        order.  If the file is missing, empty, or malformed an empty dictionary
        is returned.
    """

    if not os.path.exists(path) or os.path.getsize(path) == 0:
        # Missing or empty Masscan output means there is nothing to merge.
        return {}

    try:
        with open(path, "r", encoding="utf-8") as file:
            masscan_results = json.load(file)
    except Exception:
        # Any parsing failure is treated as "no results" to keep the workflow
        # resilient against partially generated data.
        return {}

    hosts: Dict[str, Dict[str, List[int]]] = {}

    if isinstance(masscan_results, list):
        for entry in masscan_results:
            ip = entry.get("ip")
            if not ip:
                continue

            # Collect ports only when the dictionary contains the "port" key.
            ports = [
                port_info.get("port")
                for port_info in entry.get("ports", [])
                if "port" in port_info
            ]

            # Filter out non-numeric values, deduplicate, and store the sorted
            # list for the current host.
            hosts[ip] = {
                "masscan_ports": sorted(
                    {
                        int(port)
                        for port in ports
                        if isinstance(port, int) or str(port).isdigit()
                    }
                )
            }

    return hosts


def parse_smrib_json(path: str) -> Dict[str, Dict[str, List[int]]]:
    """Read an smrib JSON export and return a mapping of IPs to port numbers.

    The smrib tool does not have an officially documented JSON schema, so the
    parser is intentionally defensive.  It walks the decoded JSON structure and
    attempts to associate any values that look like IP addresses with port
    numbers contained in the same object hierarchy.

    Parameters
    ----------
    path:
        Location of the JSON log produced by ``smrib.py``.

    Returns
    -------
    dict
        Keys are IP address strings.  Values are dictionaries containing a
        ``smrib_ports`` list with the discovered port numbers in ascending
        order.
    """

    if not os.path.exists(path) or os.path.getsize(path) == 0:
        return {}

    try:
        with open(path, "r", encoding="utf-8") as file:
            data = json.load(file)
    except Exception:
        return {}

    hosts: Dict[str, Dict[str, List[int]]] = {}

    def _collect_ports(value: object) -> List[int]:
        ports: List[int] = []
        if isinstance(value, list):
            for item in value:
                ports.extend(_collect_ports(item))
        elif isinstance(value, dict):
            if "port" in value and value["port"] is not None:
                candidate = value["port"]
                if isinstance(candidate, int):
                    ports.append(candidate)
                elif isinstance(candidate, str) and candidate.isdigit():
                    ports.append(int(candidate))
            for key in ("ports", "services", "open_ports"):
                if key in value:
                    ports.extend(_collect_ports(value[key]))
            # Fall back to scanning all nested values.
            for nested in value.values():
                ports.extend(_collect_ports(nested))
        elif isinstance(value, int):
            ports.append(value)
        elif isinstance(value, str) and value.isdigit():
            ports.append(int(value))
        return ports

    def _traverse(node: object, current_ip: Optional[str] = None) -> None:
        if isinstance(node, dict):
            ip_value: Optional[str] = None
            for key in ("ip", "host", "address", "ip_address"):
                candidate = node.get(key)
                if isinstance(candidate, str) and candidate.strip():
                    candidate = candidate.strip()
                    try:
                        ipaddress.ip_address(candidate)
                        ip_value = candidate
                        break
                    except ValueError:
                        continue

            ports = _collect_ports(node.get("ports")) if "ports" in node else []
            if not ports and "services" in node:
                ports = _collect_ports(node["services"])
            if not ports and "open_ports" in node:
                ports = _collect_ports(node["open_ports"])
            if not ports and "port" in node:
                ports = _collect_ports(node["port"])

            use_ip = ip_value or current_ip
            if use_ip and ports:
                entry = hosts.setdefault(use_ip, {"smrib_ports": []})
                entry.setdefault("smrib_ports", [])
                entry["smrib_ports"] = sorted(
                    set(entry["smrib_ports"]) | set(ports)
                )

            for value in node.values():
                _traverse(value, ip_value or current_ip)
        elif isinstance(node, list):
            for item in node:
                _traverse(item, current_ip)

    _traverse(data)

    return hosts


def parse_nmap_dir(nmap_dir: str) -> Dict[str, Dict[str, Iterable[Dict[str, Optional[str]]]]]:
    """Load one or more Nmap XML files and extract host information.

    Parameters
    ----------
    nmap_dir:
        Directory containing the XML output created by Nmap.  Only ``.xml``
        files are processed.

    Returns
    -------
    dict
        A dictionary keyed by IP address with values that contain three keys:
        ``nmap_ports`` (list of port dictionaries), ``hostnames`` (list of
        resolved hostnames), and ``os`` (best-effort OS match string).
    """

    inventory: Dict[str, Dict[str, Iterable[Dict[str, Optional[str]]]]] = {}

    if not os.path.isdir(nmap_dir):
        # Without an Nmap directory there is nothing to parse, so return early.
        return inventory

    for fname in os.listdir(nmap_dir):
        if not fname.endswith(".xml"):
            continue

        xml_file = os.path.join(nmap_dir, fname)

        try:
            root = ET.parse(xml_file).getroot()
        except Exception:
            # Skip files that cannot be parsedâ€”corrupt or incomplete files
            # should not interrupt the aggregation process.
            continue

        for host in root.findall("host"):
            address: Optional[str] = None

            # Prefer IPv4/IPv6 addresses, but fall back to any available
            # address if none are explicitly tagged.
            for address_node in host.findall("address"):
                if address_node.get("addrtype") in ("ipv4", "ipv6"):
                    address = address_node.get("addr")
                    break

            if not address:
                addresses = host.findall("address")
                if addresses:
                    address = addresses[0].get("addr")

            if not address:
                continue

            info = inventory.setdefault(
                address,
                {"nmap_ports": [], "hostnames": [], "os": None},
            )

            for hostname in host.findall("hostnames/hostname"):
                name = hostname.get("name")
                if name and name not in info["hostnames"]:
                    info["hostnames"].append(name)

            osnode = host.find("os/osmatch")
            if osnode is not None and not info.get("os"):
                info["os"] = osnode.get("name")

            for port in host.findall("ports/port"):
                portnum = int(port.get("portid"))
                proto = port.get("protocol")

                state_element = port.find("state")
                state = state_element.get("state") if state_element is not None else None

                service = port.find("service")
                service_name = service.get("name") if service is not None else None

                version: Optional[str] = None
                if service is not None:
                    version_components = [
                        service.get(key)
                        for key in ("product", "version", "extrainfo")
                        if service.get(key)
                    ]
                    if version_components:
                        version = " ".join(version_components)

                info["nmap_ports"].append(
                    {
                        "port": portnum,
                        "proto": proto,
                        "state": state,
                        "service": service_name,
                        "version": version,
                    }
                )

    return inventory


class HarvesterFinding(NamedTuple):
    """Container describing a host discovered by theHarvester."""

    hostname: str
    ip: Optional[str]


HARVESTER_DOMAIN_KEYS: Sequence[str] = (
    "domain",
    "target",
    "dns_domain",
    "query",
    "search",
)

HARVESTER_HOSTNAME_KEYS: Sequence[str] = (
    "host",
    "hostname",
    "domain",
    "name",
    "target",
    "fqdn",
)


def _looks_like_ip(value: str) -> bool:
    try:
        ipaddress.ip_address(value)
    except ValueError:
        return False
    return True


def _normalise_hostname(raw: str) -> Optional[str]:
    candidate = raw.strip()
    if not candidate:
        return None

    # Remove URL schemes and fragments that occasionally appear in reports.
    candidate = re.sub(r"^[a-z]+://", "", candidate, flags=re.IGNORECASE)
    candidate = candidate.split("/")[0]
    candidate = candidate.split(" ")[0]

    # theHarvester may append port or IP information using a colon.  If the
    # suffix is an IP address we strip it so the hostname can be matched later.
    hostname, sep, suffix = candidate.partition(":")
    if sep and _looks_like_ip(suffix):
        candidate = hostname
    else:
        candidate = candidate.rstrip(":")

    candidate = candidate.strip().strip(".")
    candidate = candidate.lstrip("*.")
    if not candidate or " " in candidate or "@" in candidate:
        return None

    # Hostnames should have at least one dot to avoid collecting usernames or
    # other noise.
    if "." not in candidate:
        return None

    if _looks_like_ip(candidate):
        return None

    return candidate.lower()


def _extract_hostname_and_ip(entry: object) -> Optional[HarvesterFinding]:
    hostname: Optional[str] = None
    address: Optional[str] = None

    if isinstance(entry, str):
        normalised = _normalise_hostname(entry)
        if normalised:
            hostname = normalised

        # Attempt to recover an IP address when present as a suffix.
        if ":" in entry:
            maybe_host, _, maybe_ip = entry.partition(":")
            if maybe_host and maybe_ip and _looks_like_ip(maybe_ip.strip()):
                address = maybe_ip.strip()

    elif isinstance(entry, dict):
        for key in HARVESTER_HOSTNAME_KEYS:
            value = entry.get(key)
            if isinstance(value, str):
                hostname = _normalise_hostname(value)
                if hostname:
                    break

        ip_value = entry.get("ip") or entry.get("ip_address")
        if isinstance(ip_value, str) and _looks_like_ip(ip_value.strip()):
            address = ip_value.strip()

    if not hostname:
        return None

    return HarvesterFinding(hostname=hostname, ip=address)


def _extract_domain_from_metadata(metadata: object, fallback: str) -> str:
    if isinstance(metadata, dict):
        for key in HARVESTER_DOMAIN_KEYS:
            value = metadata.get(key)
            if isinstance(value, str) and value.strip():
                return value.strip().lower()
            if isinstance(value, dict):
                nested = _extract_domain_from_metadata(value, fallback)
                if nested:
                    return nested

    if isinstance(metadata, str) and metadata.strip():
        return metadata.strip().lower()

    return fallback.lower()


def parse_harvester_dir(harv_dir: str) -> Dict[str, List[HarvesterFinding]]:
    """Read theHarvester outputs and associate domains with discovered hosts."""

    mapping: Dict[str, List[HarvesterFinding]] = {}

    if not os.path.isdir(harv_dir):
        return mapping

    for fname in os.listdir(harv_dir):
        path = os.path.join(harv_dir, fname)

        if os.path.isdir(path):
            continue

        findings: List[HarvesterFinding] = []
        domain_hint = os.path.splitext(fname)[0].lower()

        try:
            with open(path, "r", encoding="utf-8", errors="ignore") as file:
                content = file.read()
        except Exception:
            continue

        domain_name = domain_hint

        # Prefer parsing JSON when available as it contains structured data.
        try:
            data = json.loads(content)
        except json.JSONDecodeError:
            data = None

        if isinstance(data, dict):
            domain_name = _extract_domain_from_metadata(data, domain_hint)

            raw_hosts: Sequence[object] = []
            if isinstance(data.get("hosts"), list):
                raw_hosts = data["hosts"]
            elif isinstance(data.get("hosts"), dict):
                raw_hosts = list(data["hosts"].values())

            # Some outputs (e.g., JSON exported from the UI) nest host entries
            # inside a ``data`` key.
            if not raw_hosts and isinstance(data.get("data"), dict):
                nested_hosts = data["data"].get("hosts")
                if isinstance(nested_hosts, list):
                    raw_hosts = nested_hosts

            for entry in raw_hosts:
                finding = _extract_hostname_and_ip(entry)
                if finding:
                    findings.append(finding)

            # theHarvester may expose additional hostnames under other keys
            # depending on the selected modules.  Traverse the full JSON
            # structure to discover any remaining host-like artefacts.
            stack = [data]
            while stack:
                current = stack.pop()
                finding = _extract_hostname_and_ip(current)
                if finding:
                    findings.append(finding)

                if isinstance(current, dict):
                    stack.extend(current.values())
                elif isinstance(current, list):
                    stack.extend(current)

        if not findings:
            # Fallback to the previous line-by-line heuristic when JSON parsing
            # either failed or did not reveal any hostnames.
            for line in content.splitlines():
                finding = _extract_hostname_and_ip(line)
                if finding:
                    findings.append(finding)

        if not findings:
            continue

        deduped: Dict[str, HarvesterFinding] = {}
        for finding in findings:
            existing = deduped.get(finding.hostname)
            if existing is None or (existing.ip is None and finding.ip):
                deduped[finding.hostname] = finding

        mapping[domain_name] = sorted(deduped.values(), key=lambda item: item.hostname)

    return mapping


def build_inventory(
    nmap_inv: Dict[str, Dict[str, Iterable[Dict[str, Optional[str]]]]],
    masscan_inv: Dict[str, Dict[str, List[int]]],
    smrib_inv: Dict[str, Dict[str, List[int]]],
    harv_map: Dict[str, List[HarvesterFinding]],
) -> Dict[str, Dict[str, Optional[Iterable]]]:
    """Merge the tool-specific outputs into a unified inventory structure."""

    inventory: Dict[str, Dict[str, Optional[Iterable]]] = {}

    # Start with Nmap results because they provide the richest context (ports,
    # services, hostnames, and OS detection).
    for ip, data in nmap_inv.items():
        inventory[ip] = {
            "ip": ip,
            "open_ports": sorted(
                {
                    port_entry["port"]
                    for port_entry in data.get("nmap_ports", [])
                    if port_entry.get("state") == "open"
                }
            ),
            "services": [
                {
                    "port": port_entry["port"],
                    "service": port_entry.get("service"),
                    "version": port_entry.get("version"),
                }
                for port_entry in data.get("nmap_ports", [])
            ],
            "hostnames": sorted(set(data.get("hostnames", []))),
            "os": data.get("os"),
        }

    # Enrich the inventory with Masscan results to ensure fast-scan findings are
    # not lost, even if Nmap did not see those hosts.
    for ip, masscan_data in masscan_inv.items():
        if ip not in inventory:
            inventory[ip] = {
                "ip": ip,
                "open_ports": masscan_data.get("masscan_ports", []),
                "services": [],
                "hostnames": [],
                "os": None,
            }
        else:
            inventory[ip]["open_ports"] = sorted(
                set(inventory[ip].get("open_ports", []))
                | set(masscan_data.get("masscan_ports", []))
            )

    # Merge smrib discoveries so that ports identified by that scanner are
    # reflected even if Nmap and Masscan missed them.
    for ip, smrib_data in smrib_inv.items():
        if ip not in inventory:
            inventory[ip] = {
                "ip": ip,
                "open_ports": smrib_data.get("smrib_ports", []),
                "services": [],
                "hostnames": [],
                "os": None,
            }
        else:
            inventory[ip]["open_ports"] = sorted(
                set(inventory[ip].get("open_ports", []))
                | set(smrib_data.get("smrib_ports", []))
            )

    # theHarvester focuses on domain names; use its discoveries to supplement
    # hostnames for every IP address.
    for domain, findings in harv_map.items():
        domain_lower = domain.lower()
        domain_suffix = f".{domain_lower}"

        for finding in findings:
            targets: List[str] = []

            if finding.ip and finding.ip in inventory:
                targets = [finding.ip]
            else:
                for ip, entry in inventory.items():
                    hostnames = [name.lower() for name in entry.get("hostnames", [])]
                    if any(
                        hn == domain_lower or hn.endswith(domain_suffix)
                        for hn in hostnames
                    ):
                        targets.append(ip)

            for target_ip in targets:
                entry = inventory[target_ip]
                if finding.hostname not in entry["hostnames"]:
                    entry["hostnames"].append(finding.hostname)

    for entry in inventory.values():
        entry["hostnames"] = sorted(set(entry.get("hostnames", [])))

    return inventory


def export_json(inv: Dict[str, Dict[str, Optional[Iterable]]], outpath: str) -> None:
    """Persist the inventory as JSON so downstream tooling can reuse it."""

    with open(outpath, "w", encoding="utf-8") as file:
        json.dump(list(inv.values()), file, indent=2)


def export_csv(inv: Dict[str, Dict[str, Optional[Iterable]]], outpath: str) -> None:
    """Write a tabular view of the inventory that is easy to inspect manually."""

    with open(outpath, "w", newline="", encoding="utf-8") as file:
        writer = csv.writer(file)
        writer.writerow(["ip", "hostnames", "os", "open_ports", "services"])

        for ip, entry in inv.items():
            hostnames = ";".join(entry.get("hostnames", []))
            osname = entry.get("os") or ""
            ports = ";".join(str(port) for port in entry.get("open_ports", []))
            services = ";".join(
                f"{service.get('port')}:{service.get('service') or ''}:{service.get('version') or ''}"
                for service in entry.get("services", [])
            )

            writer.writerow([ip, hostnames, osname, ports, services])


def main() -> None:
    """Script entry point that wires together parsing, aggregation, and export."""

    parser = argparse.ArgumentParser(
        description=(
            "Merge Masscan, Nmap, and theHarvester outputs into JSON and CSV "
            "inventory files."
        )
    )
    parser.add_argument("--nmap-dir", default="out/nmap", help="Directory with Nmap XML files.")
    parser.add_argument("--masscan-json", default="out/masscan.json", help="Masscan JSON results file.")
    parser.add_argument(
        "--smrib-json",
        default="out/smrib.json",
        help="JSON log produced by smrib.py (if used).",
    )
    parser.add_argument(
        "--harv-dir",
        default="out/harvester",
        help="Directory containing theHarvester text exports.",
    )
    parser.add_argument("--out-json", default="out/inventory.json", help="Path for the merged JSON output.")
    parser.add_argument("--out-csv", default="out/inventory.csv", help="Path for the merged CSV output.")

    args = parser.parse_args()

    masscan_results = parse_masscan_json(args.masscan_json)
    smrib_results = parse_smrib_json(args.smrib_json)
    nmap_results = parse_nmap_dir(args.nmap_dir)
    harvester_results = parse_harvester_dir(args.harv_dir)

    inventory = build_inventory(
        nmap_results,
        masscan_results,
        smrib_results,
        harvester_results,
    )

    os.makedirs(os.path.dirname(args.out_json), exist_ok=True)
    export_json(inventory, args.out_json)
    export_csv(inventory, args.out_csv)

    print(f"Wrote {args.out_json} and {args.out_csv}")

    def _sort_key(ip: str) -> tuple:
        try:
            return (0, ipaddress.ip_address(ip))
        except ValueError:
            return (1, ip)

    for ip in sorted(inventory, key=_sort_key):
        ports = sorted({port for port in inventory[ip].get("open_ports", []) if port is not None})
        if not ports:
            continue
        for port in ports:
            print(f"{ip} {port}")


if __name__ == "__main__":
    main()
